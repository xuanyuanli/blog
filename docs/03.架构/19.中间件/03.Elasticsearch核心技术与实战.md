
## 为何学习 Elasticsearch

Elasticsearch 是一款开源的、分布式的、实时的搜索与数据分析引擎，它具有高度的可伸缩性、灵活性和实时性，能够帮助用户在大量数据中进行高效的搜索、分析和数据挖掘。在当今这个信息爆炸的时代，学习 Elasticsearch 成为了一项重要的技能，原因如下：

1. **高性能搜索**：Elasticsearch 基于 Apache Lucene 构建，提供了全文搜索、模糊匹配和多语言支持等功能，使得用户可以在海量数据中快速找到相关信息。

2. **实时数据分析**：Elasticsearch 支持实时的数据分析和聚合，用户可以在短时间内对大量数据进行处理，以获得有价值的洞察和业务智能。

3. **分布式与可伸缩性**：Elasticsearch 具有分布式架构，能够在多台服务器上横向扩展，轻松应对不断增长的数据量和查询负载。

4. **丰富的生态系统**：Elasticsearch 是 ELK（Elasticsearch、Logstash 和 Kibana）技术栈的核心组件，结合其他组件可以实现日志分析、监控、数据可视化等多种场景。

5. **广泛的应用领域**：Elasticsearch 在各种领域都有广泛的应用，如电商、金融、医疗、教育等，学习 Elasticsearch 有助于提高在这些领域的工作技能。

<!-- more -->

## 发展历史

Elasticsearch 的发展历程如下：

- **2004年**：Doug Cutting 和 Mike Cafarella 开始开发 Apache Lucene，一款开源的全文搜索引擎库，后来成为 Elasticsearch 的基础技术。

- **2010年**：Shay Banon 创立 Elasticsearch 项目，以解决分布式环境下的搜索和分析问题。他的启发源自于他为家人开发的食谱搜索应用。

- **2012年**：Elasticsearch 成立公司，开始提供商业支持和服务。

- **2013年**：公司推出 Logstash 和 Kibana，与 Elasticsearch 配合使用，形成了 ELK 技术栈。

- **2015年**：Elasticsearch 公司正式更名为 Elastic，继续扩展其产品线，包括 Beats、Elastic Cloud 等。

- **2018年**：Elastic 在纳斯达克上市，股票代码为 "ESTC"。

从诞生至今，Elasticsearch 已经成为全球最受欢迎的搜索与分析引擎之一，并得到了众多知名企业和开发者的广泛认可与支持。

## 基础概念
### 集群

集群是由多个 Elasticsearch 节点组成的一个完整的系统，这些节点共同工作以提供分布式搜索和数据分析功能。集群具有以下特点：

1. **高可用性**：集群中的节点可以相互协作，当某个节点出现故障时，其他节点会接管其工作，确保系统的正常运行。

2. **数据分布**：集群可以将数据分布在多个节点上，实现数据的负载均衡和性能优化。

3. **水平扩展**：当集群需要处理更多的数据和查询时，可以通过添加节点来提高集群的处理能力。

### 节点

节点是 Elasticsearch 集群中的一个独立的服务器，负责处理数据和执行搜索、分析任务。节点有以下类型：

1. **主节点**：负责管理集群的元数据，如索引的创建、删除、映射配置等。一个集群只能有一个主节点。

2. **数据节点**：负责存储数据和执行数据相关的操作，如搜索、分析、聚合等。

3. **协调节点**：负责分发客户端的请求到各个数据节点，并汇总、返回结果。

4. **摄取节点**：负责预处理数据，如数据转换、过滤等，然后将处理后的数据传递给数据节点。

### 分片

分片是 Elasticsearch 的一个核心概念，它可以将一个大的索引拆分成多个较小的、独立的部分。分片有以下优点：

1. **水平扩展**：通过增加分片数量，可以在更多节点上分布数据，提高查询和写入的性能。

2. **容错性**：当某个分片所在的节点出现故障时，其他节点上的分片仍然可以提供服务。

3. **并行处理**：分片可以让多个节点同时处理数据，提高查询和分析的速度。

### 副本

副本是分片的一个备份，它可以提高数据的可用性和容错性。副本有以下作用：

1. **容错性**：当一个分片所在的节点发生故障时，副本可以替代原始分片继续提供服务。

2. **负载均衡**：副本可以在多个节点上分布，提供搜索和读取的负载均衡。

3. **高可用性**：副本可以确保在节点故障或网络分区等异常情况下，数据仍然可以被访问。

在设计 Elasticsearch 集群时，需要根据实际需求和场景来合理配置分片和副本的数量，以达到最佳的性能和可靠性。

### 索引

在 Elasticsearch 中，索引是一个用于存储和检索文档的逻辑空间。它类似于传统数据库中的表，是组织和管理数据的基本单位。索引具有以下特点：

1. **映射**：索引定义了数据的结构和类型，称为映射。映射包括字段名、数据类型、分析器等属性，可以帮助 Elasticsearch 更高效地处理和查询数据。

2. **分片与副本**：索引由多个分片组成，每个分片可以有一个或多个副本。分片和副本可以在集群的多个节点上分布，提高查询和写入的性能，以及数据的可用性和容错性。

3. **动态索引**：Elasticsearch 支持动态创建索引和映射，当接收到新的文档时，可以根据预定义的规则自动创建对应的索引和映射。

4. **别名**：为了方便管理和查询，可以为索引设置别名。使用别名可以让你在不改变查询逻辑的情况下，轻松地对索引进行重建或者迁移。

### 文档

文档是 Elasticsearch 中存储和处理的基本数据单位，它代表了一个具体的实体或对象。文档具有以下特点：

1. **JSON 格式**：文档使用 JSON 格式表示，这是一种轻量级、易于阅读和编写的数据交换格式。JSON 格式使得文档可以包含复杂的数据结构，如数组、嵌套对象等。

2. **唯一标识**：每个文档都有一个唯一的标识，称为 `_id`。你可以在创建文档时自定义 `_id`，或者让 Elasticsearch 自动生成。

3. **元数据**：除了用户定义的数据之外，文档还包含一些元数据，如 `_index`（所属索引）、`_type`（文档类型，已经在 Elasticsearch 7.x 中废弃）和 `_version`（文档版本）等。

4. **全文搜索**：Elasticsearch 可以对文档的内容进行全文搜索、模糊匹配和多语言支持等功能，使得用户可以在大量数据中快速找到相关信息。

在 Elasticsearch 中，你可以执行各种操作来创建、更新、删除和查询文档，如使用 RESTful API、Java 客户端或者其他语言的客户端库。同时，Elasticsearch 还提供了一系列高级功能，如排序、过滤、聚合和地理位置搜索等，帮助你更有效地处理和分析数据。

在 Elasticsearch 中，setting 和 mapping 是用于定义和配置索引结构的两个重要概念。

### 倒排索引
Elasticsearch使用倒排索引（Inverted Index）来快速查询文档。倒排索引是一个数据结构，它存储了每个术语（Term）在哪些文档中出现过。对于一个查询语句，Elasticsearch会在倒排索引中寻找关键词所在的文档，然后对这些文档进行计算，得出符合查询条件的结果。

倒排索引的构建过程主要包括以下两个步骤：

分词（Tokenization）：将文档中的字符串切分成一个个单词或术语。这里的单词或术语是指被定义了的一些基本单位，例如英文字母或数字等。

建立词条表（Term Dictionary）：对于文档中的每个术语，建立一张该术语出现的文档列表。这样就可以查找包含某个词语的文档。

在倒排索引中，术语是一个基本单位，也称为Term。每个Term都有一个Document Frequency（DF），即该术语出现在的文档数。此外，每个Term在每个文档中可能会出现多次，为了存储更多信息，索引还会维护一个Positions List，用于记录Term在文档中的位置信息。

倒排索引使得Elasticsearch能够以较小的代价（O(1)级别）查找包含某个术语的文档列表。倒排索引也允许Elasticsearch支持高效的全文搜索、短语搜索等复杂查询。可以说，倒排索引是Elasticsearch的核心特性之一，也是其高效检索的关键所在。


### Setting

Setting 是用于配置索引参数和行为的设置。它包括了诸如分片数量、副本数量、分词器配置等方面的内容。Setting 可以在创建索引时进行配置，也可以在索引创建后进行部分修改。以下是一些常见的 setting 配置：

1. **分片数量**：设置索引的主分片数量，这个值在索引创建后不能修改。例如：`"number_of_shards": 5`。

2. **副本数量**：设置每个分片的副本数量，这个值在索引创建后可以修改。例如：`"number_of_replicas": 1`。

3. **分词器配置**：配置自定义分词器、分词过滤器等，以支持特定语言或业务场景的文本分析。例如，可以定义一个用于处理英文文本的自定义分词器。

通过设置合适的 setting 参数，可以根据具体应用场景和需求对索引进行优化，以提高查询性能和数据可用性。

### Mapping

Mapping 是用于定义索引中文档的字段结构和属性的映射。它相当于传统关系型数据库中的表结构定义。Mapping 包括字段名、字段类型、分析器等信息。以下是一些常见的 mapping 配置：

1. **字段类型**：指定字段的数据类型，如字符串、整数、日期等。例如：`"title": { "type": "text" }`。

2. **字段分析器**：指定字段的分析器，用于处理全文搜索的文本分割和分析。例如：`"description": { "type": "text", "analyzer": "english" }`。

3. **嵌套对象**：定义嵌套的对象结构，以支持更复杂的数据模型。例如，一个文档可以包含一个作者对象，该对象包含姓名和电子邮件等属性。

4. **多字段**：可以为同一个字段定义多个子字段，以支持不同的查询和分析需求。例如，一个字段可以同时支持全文搜索和精确匹配。

Mapping 的配置对于 Elasticsearch 的查询性能和搜索结果准确性至关重要。一个合适的 mapping 设置可以提高查询效率，减少误匹配和无关结果。

### 示例
以下是一个使用 REST API 创建 Elasticsearch 索引的示例，其中包括了 setting 和 mapping 的设置。在本例中，我们创建一个名为 "blog" 的索引。

```
PUT /blog
{
  "settings": {
    "number_of_shards": 1,
    "number_of_replicas": 1,
    "analysis": {
      "analyzer": {
        "my_custom_analyzer": {
          "type": "custom",
          "tokenizer": "standard",
          "filter": [
            "lowercase",
            "asciifolding"
          ]
        }
      }
    }
  },
  "mappings": {
    "properties": {
      "title": {
        "type": "text",
        "analyzer": "my_custom_analyzer"
      },
      "content": {
        "type": "text",
        "analyzer": "my_custom_analyzer"
      },
      "publish_date": {
        "type": "date"
      },
      "tags": {
        "type": "keyword"
      },
      "author": {
        "properties": {
          "name": {
            "type": "text"
          },
          "email": {
            "type": "keyword"
          }
        }
      }
    }
  }
}
```

在这个例子中：

1. 在 `settings` 部分，我们设置了索引的分片数（`number_of_shards`）为 1，副本数（`number_of_replicas`）为 1。我们还自定义了一个名为 "my_custom_analyzer" 的分析器，它使用标准分词器（`standard`）和两个过滤器：小写过滤器（`lowercase`）和 ASCII 折叠过滤器（`asciifolding`）。

2. 在 `mappings` 部分，我们定义了文档的字段结构和类型。包括：`title`（文本类型，使用自定义分析器）、`content`（文本类型，使用自定义分析器）、`publish_date`（日期类型）、`tags`（关键词类型）以及一个嵌套的 `author` 对象（包含 `name` 和 `email` 字段）。

通过这个示例，你可以看到如何使用 REST API 来设置 Elasticsearch 索引的 setting 和 mapping。

示例中setting包含了分片和副本数，上面章节已经有基本介绍。还用到mapping配置和分析器，后面内容会讲到。

## 文档的CURD

1. **创建（Create）**：向 "blog" 索引中添加一篇博客文章：

```
POST /blog/_doc/
{
  "title": "Elasticsearch: Getting Started",
  "content": "This is a beginner's guide to Elasticsearch.",
  "publish_date": "2023-04-25T12:00:00Z",
  "tags": ["Elasticsearch", "Guide"],
  "author": {
    "name": "John Doe",
    "email": "john.doe@example.com"
  }
}
```

2. **读取（Read）**：获取刚刚创建的博客文章。假设文章的 `_id` 为 `1`：

```
GET /blog/_doc/1
```

3. **更新（Update）**：更新博客文章的部分内容。在这个例子中，我们为文章添加一个新的标签：

```
POST /blog/_doc/1/_update
{
  "doc": {
    "tags": ["Elasticsearch", "Guide", "Tutorial"]
  }
}
```

另外，你还可以使用脚本来更新文档的内容：

```
POST /blog/_doc/1/_update
{
  "script": {
    "source": "ctx._source.tags.add(params.new_tag)",
    "params": {
      "new_tag": "Search"
    }
  }
}
```

4. **删除（Delete）**：删除博客文章。假设文章的 `_id` 为 `1`：

```
DELETE /blog/_doc/1
```

### 更新setting
在 Elasticsearch 中，一旦创建了索引，某些 settings 和 mapping 的属性无法再进行修改。

可以动态修改的setting设置通常与分片、副本或搜索相关。要更新 settings，请使用 _settings API。以下是一个示例：
```
PUT /my_index/_settings
{
  "index": {
    "number_of_replicas": 2
  }
} 
```

关于动态和静态属性，参考[Index Setting](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/index-modules.html#index-modules-settings)

## 动态映射（Dynamic Mapping）和常见字段类型
Elasticsearch 提供了一种名为 Dynamic Mapping 的功能，它可以在索引文档时自动识别字段类型并创建相应的映射。这对于不确定文档结构或者想要快速尝试的场景非常有用。

对于生产环境来说，建议使用显式映射来控制字段类型和属性，以获得更精确的控制和更好的性能。

### Dynamic Mapping配置
Dynamic Mapping 的行为可以通过以下三种方式进行配置：

- `true`（默认）：允许自动创建映射。
- `false`：禁止自动创建映射，需要手动定义映射。
- `strict`：若遇到未映射的字段，会抛出异常。

要配置 Dynamic Mapping，您可以在创建索引时设置 `index.mapping.dynamic` 参数，如下所示：
```json
{
  "settings": {
    "index.mapping.dynamic": "strict"
  }
}
```

### 常见字段类型

Elasticsearch 支持多种字段类型，以满足不同类型数据的索引和查询需求。以下是一些常见的字段类型及其简要介绍：

1. **text**：用于全文搜索的字符串类型。text 字段会被分析，支持全文搜索和相关性评分。
2. **keyword**：用于精确值的字符串类型。keyword 字段不会被分析，适用于过滤、排序和聚合操作。
3. **integer**、**long**、**short**、**byte**：整数类型，分别对应 32 位、64 位、16 位和 8 位整数。
4. **float**、**double**：浮点数类型，分别对应 32 位和 64 位浮点数。
5. **date**：日期类型，支持多种日期格式。可以通过 `format` 参数自定义日期格式。
6. **boolean**：布尔类型，表示真或假的值。
7. **ip**：IP 地址类型，支持 IPv4 和 IPv6。
8. **geo_point**：地理坐标类型，用于表示地理位置。支持多种坐标表示方式，如经纬度数组、字符串和对象。
9. **nested**：嵌套类型，用于表示 JSON 对象数组。与普通对象字段不同，nested 字段会保留对象间的关系，便于进行嵌套查询。

示例：
```json
{
 "properties": {
   "comments": {
     "type": "nested",
     "properties": {
       "author": {
         "type": "text"
       },
       "content": {
         "type": "text"
       }
     }
   }
 }
}
```

### 对于null值的处理
在 Elasticsearch 中，`null` 值的处理方式取决于字段类型。下面是关于 `null` 值处理的一些概述：

1. 对于数值类型（如 `integer`、`long`、`float` 和 `double`）和布尔类型（`boolean`），Elasticsearch 会忽略包含 `null` 值的字段。这意味着在索引文档时，该字段不会被存储，且不会占用任何磁盘空间。

2. 对于字符串类型（如 `text` 和 `keyword`）以及其他复杂类型（如 `date`、`ip`、`geo_point` 和 `nested`），同样会忽略 `null` 值。该字段不会被存储在索引中，也不会对查询和聚合操作产生影响。

3. 如果需要在 Elasticsearch 中表示 `null` 值，可以使用一个特殊的值来代替，例如使用一个特殊的字符串或者数值。这种方法在进行查询和聚合时会更加灵活。例如，可以使用一个不会出现在实际数据中的字符串（如 `__NULL__`）来表示 `null` 值。

例如，在创建索引时，可以为某个字段设置一个 `null_value`，以便在接收到 `null` 值时使用指定的默认值：

```json
{
  "properties": {
    "age": {
      "type": "integer",
      "null_value": -1
    },
    "status": {
      "type": "keyword",
      "null_value": "__NULL__"
    }
  }
}
```

在这个例子中，当 `age` 字段接收到 `null` 值时，会将其替换为 `-1`；当 `status` 字段接收到 `null` 值时，会将其替换为 `__NULL__`。这样，在进行查询和聚合时，可以使用这些特殊值来表示 `null` 值。

### 更新mapping
某些 mapping 属性可以在创建索引后进行更新。这些属性通常与字段类型、索引选项或分析器相关。要更新 mapping，请使用 _mapping API。以下是一个示例：
```json
PUT /my_index/_mapping
{
  "properties": {
    "new_field": {
      "type": "text"
    },
    "existing_field": {
      "type": "keyword",
      "ignore_above": 256
    }
  }
}
```
在这个示例中，我们为索引 my_index 添加了一个名为 new_field 的新字段，并更新了现有字段 existing_field 的 ignore_above 属性。

请注意，只有部分 mapping 属性可以被更新。例如，您无法更改现有字段的类型，也无法删除现有字段。请参考：[Update mapping API](https://www.elastic.co/guide/en/elasticsearch/reference/7.17/indices-put-mapping.html)


### 重建索引
如果您需要对 settings 或 mapping 进行大规模更改，可以考虑使用 `_reindex` API 重建索引。这个 API 可以将现有索引中的数据复制到一个新的索引，并在新索引上应用新的 settings 和 mapping。在完成重建后，可以将别名切换到新索引，以实现无缝迁移。以下是一个示例：

1. 创建一个新的索引，应用新的 settings 和 mapping：

```json
PUT /new_index
{
  "settings": {
    // 新的 settings
  },
  "mappings": {
    // 新的 mapping
  }
}
```

2. 使用 `_reindex` API 将数据从旧索引复制到新索引：

```json
POST /_reindex
{
  "source": {
    "index": "old_index"
  },
  "dest": {
    "index": "new_index"
  }
}
```

3. 在完成数据迁移后，将别名切换到新索引：

```json
POST /_aliases
{
  "actions": [
    {
      "remove": {
        "index": "old_index",
        "alias": "my_alias"
      }
    },
    {
      "add": {
        "index": "new_index",
        "alias": "my_alias"
      }
    }
  ]
}
```

在这个示例中，我们先从旧索引 `old_index` 移除别名 `my_alias`，然后将别名添加到新索引 `new_index`。这样，应用程序可以继续使用别名 `my_alias` 访问新索引，而无需对其进行修改。

请注意，重建索引可能需要一定的时间，具体取决于数据量和硬件性能。在重建索引期间，您需要确保旧索引和新索引都可用，以避免对业务造成影响。


## Analyzer
在 Elasticsearch 中，Analyzer（分析器）是用于处理全文搜索中的文本分析过程的组件。文本分析是将文本数据转换成可以被倒排索引存储和查询的词项的过程。

分析器通常由三个部分组成：
- 字符过滤器（Char Filter）：字符过滤器对原始文本进行预处理，例如删除 HTML 标签、转换特殊字符等。
- 分词过滤器（Token Filter）：分词过滤器对从分词器产生的词项进行处理和过滤。常见的分词过滤器包括小写过滤器（将词项转换为小写）、停用词过滤器（去除常见的停用词，如 "and"、"the" 等）和词干提取过滤器（将词项转换为其词干，以便在搜索时匹配不同形式的单词）。
- 分词器（Tokenizer）：分词器将文本拆分成独立的词项。例如，标准分词器会将文本按空格和标点符号拆分成单词。

### 字符过滤器 使用
下面给出一些常见的Character Filters的例子，以便更好地理解它们的作用：

1. HTML Strip Char Filter

该过滤器可以去除HTML标签，将文本转换为纯文本。

```json
POST _analyze
{
  "tokenizer": "standard",
  "char_filter": [ {
    "type": "html_strip"
  }],
  "text": "<p>Hello, <b>world</b>!</p>"
}
```
输出结果为：
```json
{
  "tokens" : [
    {
      "token" : "Hello",
      "start_offset" : 3,
      "end_offset" : 8,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 13,
      "end_offset" : 22,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}
```

2. Mapping Char Filter

该过滤器可以将指定字符映射为其他字符。例如，将所有的"&"替换为"and"。

```
POST _analyze
{
  "char_filter": [
    {
      "type": "mapping",
      "mappings": [
        "& => and",
        "| => or"
      ]
    }
  ],
  "text": "apples & oranges | bananas"
}
```
输出结果为：
```
{
  "tokens" : [
    {
      "token" : "apples and oranges or bananas",
      "start_offset" : 0,
      "end_offset" : 26,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

###  分词过滤器 使用
下面给出一些常见的Token Filters的例子，以便更好地理解它们的作用：

1. Lowercase Token Filter

小写分词器会将分词结果转换为小写字母。

```
POST _analyze
{
  "tokenizer": "standard",
  "filter": ["lowercase"],
  "text": "Hello, world! "
}
```
输出结果为：
```
{
  "tokens" : [
    {
      "token" : "hello",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 7,
      "end_offset" : 12,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}

```
2. Stop Token Filter
停用词分词器会去除文本中的停用词，如“and”、“the”等，然后再进行分词和转换操作。停用词分词器可以通过以下方式进行配置：

```
POST _analyze
{
  "tokenizer": "standard",
  "filter": ["stop"],
  "text": "Elasticsearch is awesome and great."
}
```

输出结果为：
```
{
  "tokens" : [
    {
      "token" : "Elasticsearch",
      "start_offset" : 0,
      "end_offset" : 13,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "awesome",
      "start_offset" : 17,
      "end_offset" : 24,
      "type" : "<ALPHANUM>",
      "position" : 2
    },
    {
      "token" : "great",
      "start_offset" : 29,
      "end_offset" : 34,
      "type" : "<ALPHANUM>",
      "position" : 4
    }
  ]
}
```

###  分词器 使用
下面给出一些常见的Tokenizer的例子，以便更好地理解它们的作用：

1. Standard Tokenizer

标准分词器是Elasticsearch中最常用的分词器之一，它会将文本按照标点符号、空格等进行分词，同时会将文本转换为小写字母。标准分词器可以通过以下方式进行配置：

```
POST _analyze
{
  "tokenizer": "standard",
  "text": "Hello, world!"
}
```

输出结果为：
```
{
  "tokens" : [
    {
      "token" : "Hello",
      "start_offset" : 0,
      "end_offset" : 5,
      "type" : "<ALPHANUM>",
      "position" : 0
    },
    {
      "token" : "world",
      "start_offset" : 7,
      "end_offset" : 12,
      "type" : "<ALPHANUM>",
      "position" : 1
    }
  ]
}

```

2. Keyword Tokenizer

关键字分词器会将整个文本作为一个关键字进行索引，不会进行任何分词和处理操作。

```
POST _analyze
{
  "tokenizer": "keyword",
  "text": "Hello, world! Elasticsearch is awesome."
}

```

输出结果为：
```
{
  "tokens" : [
    {
      "token" : "Hello, world! Elasticsearch is awesome.",
      "start_offset" : 0,
      "end_offset" : 39,
      "type" : "word",
      "position" : 0
    }
  ]
}
```

### 自定义分析器
理解了分析器的组成部分，就可以未索引创建自定义分析器，并把自定义分析器绑定到某个字段上使用了。示例参考 `基础概念-示例` 章节

### 内置分析器
Elasticsearch 提供了许多内置的分析器：
1. `Standard Analyzer` 标准分析器是Elasticsearch中最常用的分析器之一，它会将文本按照标点符号、空格等进行分词，同时会将文本转换为小写字母。Standard Analyzer适用于一般性的全文搜索场景，可以快速地进行分词和检索。
2. `Simple Analyzer` 简单分析器是一种比标准分析器更加简单的分析器，它只会将文本按照空格进行分词，并且不会进行任何转换操作。Simple Analyzer适用于不需要考虑大小写和单复数等问题的场景，比如数字、代码等。
3. `Whitespace Analyzer` 空格分析器会将文本按照空格进行分词，不会进行任何转换操作。Whitespace Analyzer适用于不需要进行复杂分词和处理的场景，比如标签、ID等。
4. `Keyword Analyzer` 关键字分析器会将整个文本作为一个关键字进行索引，不会进行任何分词和处理操作。Keyword Analyzer适用于需要精确匹配的场景，比如产品编号、邮编等。
5. `Stop Analyzer` 停用词分析器会去除文本中的停用词，如“and”、“the”等，然后再进行分词和转换操作。Stop Analyzer适用于需要排除常用词语的场景，比如文章标题、关键字等。
6. `Pattern Analyzer` 模式分析器会按照正则表达式对文本进行分词和转换操作，可以根据需要进行自定义。Pattern Analyzer适用于需要自定义分词规则的场景，比如特殊字符、中文词语等。
7. `Language Analyzer` 语言分析器会根据文本所属的语言进行分词和转换操作，可以提高分析的准确性和效率。Elasticsearch中支持多种语言的分析器，如中文、英文、法文等。

内置分析器可以直接使用，比如`mapping`中给字段指定分析器：
``` 
  "mappings": {
    "properties": {
      "text": {
        "type": "text",
        "analyzer": "simple"
      }
    }
  }
```

### 中文分词
目前流行的库是[IK Analysis](https://github.com/medcl/elasticsearch-analysis-ik)

官网有消息用法，这里就不赘述了。

### 拼音分词
拼音分词对于繁简体搜索很有用，使用方法请查看[Pinyin Analysis](https://github.com/medcl/elasticsearch-analysis-pinyin)

## 搜索API

在本章节中，我们将详细介绍 Elasticsearch 的搜索 API，包括分页、排序、指定查询字段以及 query 查询等功能。我们将一步步探讨各种不同类型的查询，并深入了解 Elasticsearch 的强大搜索能力。

#### 分页

Elasticsearch 提供了基于 `from` 和 `size` 参数的分页功能。`from` 参数表示从哪个索引位置开始返回结果，而 `size` 参数表示返回结果的数量。例如，如果您需要从第 11 个文档开始获取 10 个文档，可以使用如下查询：

```json
{
  "from": 10,
  "size": 10,
  "query": {
    "match_all": {}
  }
}
```

#### 排序

Elasticsearch 允许您根据一个或多个字段对查询结果进行排序。要实现排序，只需在查询中添加一个 `sort` 参数。例如，以下查询将根据 `age` 字段进行升序排序：

```json
{
  "query": {
    "match_all": {}
  },
  "sort": [
    {
      "age": "asc"
    }
  ]
}
```

#### 指定查询字段

在某些情况下，您可能只对文档的某些字段感兴趣。Elasticsearch 允许您使用 `_source` 参数来指定返回的字段。例如，以下查询将只返回 `title` 和 `author` 字段：

```json
{
  "_source": ["title", "author"],
  "query": {
    "match_all": {}
  }
}
```

#### Query查询

##### Match查询

Match 查询是 Elasticsearch 中最基本的全文搜索查询。它会将查询字符串与文档中的一个或多个字段进行比较。例如，以下查询将在 `title` 字段中查找包含 “Elasticsearch” 的文档：

```json
{
  "query": {
    "match": {
      "title": "Elasticsearch"
    }
  }
}
```

##### Match_phrase查询

Match_phrase 查询用于查找包含指定短语的文档。与 Match 查询不同，Match_phrase 查询会考虑查询字符串中的单词顺序。例如，以下查询将查找 `title` 字段中包含短语 “Elasticsearch tutorial”的文档：

```json
{
  "query": {
    "match_phrase": {
      "title": "Elasticsearch tutorial"
    }
  }
}
```

##### Term查询

Term 查询用于精确匹配某个字段的值。与 Match 查询不同，Term 查询不会对查询字符串进行分析。例如，以下查询将查找 `status` 字段值为 “published”的文档：

```json
{
  "query": {
    "term": {
      "status": "published"
    }
  }
}
```

##### Bool查询

Bool 查询允许您将多个查询条件进行组合。它提供了 `must`（必须满足）、`should`（至少满足一个）、`must_not`（不能满足） 和 `filter`（过滤）四种子句来控制查询逻辑。以下是一个 Bool 查询的示例，它要求满足以下条件：`title` 字段包含 "Elasticsearch"，`author` 字段包含 "John"，但 `status` 字段不能为 "draft"：

```json
{
  "query": {
    "bool": {
      "must": [
        {
          "match": {
            "title": "Elasticsearch"
          }
        },
        {
          "match": {
            "author": "John"
          }
        }
      ],
      "must_not": [
        {
          "term": {
            "status": "draft"
          }
        }
      ]
    }
  }
}
```

#### 评分

Elasticsearch 使用相关性评分（_score）对查询结果进行排序。_score 是一个浮点数，表示文档与查询条件的匹配程度。默认情况下，Elasticsearch 使用 TF-IDF（词频-逆文档频率）和 BM25 算法计算评分。

您可以通过在查询中添加 `min_score` 参数来过滤掉评分低于指定阈值的结果。例如，以下查询将只返回评分大于等于 1.0 的文档：

```json
{
  "min_score": 1.0,
  "query": {
    "match": {
      "title": "Elasticsearch"
    }
  }
}
```

此外，您还可以使用 `function_score` 查询来自定义评分逻辑，例如根据特定字段的值对评分进行加权。以下查询将在 `title` 字段中查找包含 "Elasticsearch" 的文档，并根据 `popularity` 字段的值对评分进行加权：

```json
{
  "query": {
    "function_score": {
      "query": {
        "match": {
          "title": "Elasticsearch"
        }
      },
      "field_value_factor": {
        "field": "popularity",
        "factor": 1.2,
        "modifier": "sqrt"
      }
    }
  }
}
```

在本章节中，我们探讨了 Elasticsearch 搜索 API 的核心功能，包括分页、排序、指定查询字段以及多种查询类型。这些功能为您提供了丰富的搜索选项，帮助您快速准确地检索所需信息。在实际应用中，您可能需要根据需求组合使用这些功能，以实现更高效的搜索体验。
